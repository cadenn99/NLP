{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "929e5787-e582-497c-94aa-e3c0e1ffcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question ', 'Step1', 'Step2', 'Step3', 'Step4', 'Step5', 'Correct_Answer', 'Question_Type', 'Question_origin', 'Step_origin', '# of reasoning steps', 'Sal_NoCOT_Answer', 'Sal_NoCOT_Correctness', 'Sal_NoCOT_P', 'Sal_NoCOT_Tokens', 'Sal_COT_Answer', 'Sal_COT_Correctness', 'Sal_COT_P', 'Sal_COT_Tokens', 'Sal_NoMost_Answer', 'Sal_NoMost_Correctness', 'Sal_NoMost_P', 'Sal_NoMost_Tokens', 'Sal_MaxStep', 'Score', 'Sal_MinStep', 'Sal_NoLeast_Answer', 'Sal_NoLeast_Correctness', 'Sal_NoLeast_P', 'Sal_NoLeast_Tokens', 'Grad_NoCOT_Answer', 'Grad_NoCOT_Correctness', 'Grad_NoCOT_P', 'Grad_NoCOT_Tokens', 'Grad_COT_Answer', 'Grad_COT_Correctness', 'Grad_COT_P', ' Grad_COT_Tokens', 'Grad_MaxStep', 'Score.1', 'Grad_NoMost_Answer', 'Grad_NoMost_Correctness', 'Grad_NoMost_P', 'Grad_NoMost_Tokens']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"C:/Users/Gebruiker/Downloads/DiffColNames2.csv\")\n",
    "# Show the first few rows\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8be35eb2-b366-4104-94f3-0d50c63a649d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Saliency - Sal_NoCOT_Correctness  \\\n",
      "Question_Type                                       \n",
      "Math                                         0.52   \n",
      "Multiple Choice                              0.92   \n",
      "Open Ended                                   0.72   \n",
      "T/F                                          0.68   \n",
      "\n",
      "                 Saliency - Sal_COT_Correctness  \\\n",
      "Question_Type                                     \n",
      "Math                                       1.00   \n",
      "Multiple Choice                            1.00   \n",
      "Open Ended                                 0.84   \n",
      "T/F                                        0.84   \n",
      "\n",
      "                 Saliency - Sal_NoMost_Correctness  \\\n",
      "Question_Type                                        \n",
      "Math                                          0.68   \n",
      "Multiple Choice                               0.96   \n",
      "Open Ended                                    0.76   \n",
      "T/F                                           0.72   \n",
      "\n",
      "                 Saliency - Sal_NoLeast_Correctness  \\\n",
      "Question_Type                                         \n",
      "Math                                           1.00   \n",
      "Multiple Choice                                0.96   \n",
      "Open Ended                                     0.88   \n",
      "T/F                                            0.88   \n",
      "\n",
      "                 Gradient - Grad_NoCOT_Correctness  \\\n",
      "Question_Type                                        \n",
      "Math                                      0.500000   \n",
      "Multiple Choice                           0.928571   \n",
      "Open Ended                                0.840000   \n",
      "T/F                                       0.560000   \n",
      "\n",
      "                 Gradient - Grad_COT_Correctness  \\\n",
      "Question_Type                                      \n",
      "Math                                        1.00   \n",
      "Multiple Choice                             1.00   \n",
      "Open Ended                                  0.88   \n",
      "T/F                                         0.68   \n",
      "\n",
      "                 Gradient - Grad_NoMost_Correctness  \n",
      "Question_Type                                        \n",
      "Math                                       0.928571  \n",
      "Multiple Choice                            1.000000  \n",
      "Open Ended                                 0.840000  \n",
      "T/F                                        0.720000  \n"
     ]
    }
   ],
   "source": [
    "# Define the relevant columns for correctness for both Saliency and Gradients\n",
    "saliency_columns = [\n",
    "    'Sal_NoCOT_Correctness', 'Sal_COT_Correctness', 'Sal_NoMost_Correctness', 'Sal_NoLeast_Correctness'\n",
    "]\n",
    "gradient_columns = [\n",
    "    'Grad_NoCOT_Correctness', 'Grad_COT_Correctness', 'Grad_NoMost_Correctness'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to hold the results\n",
    "proportions = {}\n",
    "\n",
    "# Calculate the proportion of correct answers for saliency conditions\n",
    "for col in saliency_columns:\n",
    "    saliency_correct = df.groupby('Question_Type')[col].mean()  # Mean gives the proportion of correct answers\n",
    "    proportions[f\"Saliency - {col}\"] = saliency_correct\n",
    "\n",
    "# Calculate the proportion of correct answers for gradient conditions\n",
    "for col in gradient_columns:\n",
    "    gradient_correct = df.groupby('Question_Type')[col].mean()  # Mean gives the proportion of correct answers\n",
    "    proportions[f\"Gradient - {col}\"] = gradient_correct\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier inspection\n",
    "proportion_df = pd.DataFrame(proportions)\n",
    "\n",
    "# Display the result\n",
    "print(proportion_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621ff08-a52e-4f78-9895-a96bba67b6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 5 question types: ['T/F' 'Multiple Choice' 'Open Ended' 'Math' nan]\n",
      "\n",
      "\n",
      "--- Analysis for Question Type: T/F ---\n",
      "Configuration Attribution COT_Type Accuracy Mean_P_Correct Mean_P_Incorrect P_Diff Correlation P_Value  Valid_Samples  Dropped_Samples\n",
      "     Grad_COT        Grad      COT   68.00%         0.9103           0.6093 0.3010       0.543   0.005             25                0\n",
      "  Grad_NoMost        Grad   NoMost   72.00%         0.9113           0.7045 0.2068       0.423   0.035             25                0\n",
      "   Grad_NoCOT        Grad    NoCOT   56.00%         0.8242           0.6571 0.1671       0.335   0.102             25                0\n",
      "   Sal_NoMost         Sal   NoMost   72.00%         0.9289           0.7845 0.1444       0.378   0.063             25                0\n",
      "    Sal_NoCOT         Sal    NoCOT   68.00%         0.7704           0.7232 0.0472       0.090   0.667             25                0\n",
      "      Sal_COT         Sal      COT   84.00%         0.8476           0.8001 0.0476       0.078   0.710             25                0\n",
      "  Sal_NoLeast         Sal  NoLeast   88.00%         0.9319           0.9178 0.0141       0.036   0.864             25                0\n",
      "\n",
      "\n",
      "--- Analysis for Question Type: Multiple Choice ---\n",
      "Warning: Sal_COT for Multiple Choice has insufficient data for correlation analysis\n",
      "Warning: Grad_COT for Multiple Choice has insufficient data for correlation analysis\n",
      "Warning: Grad_NoMost for Multiple Choice has insufficient data for correlation analysis\n",
      "Configuration Attribution COT_Type Accuracy Mean_P_Correct Mean_P_Incorrect P_Diff Correlation P_Value  Valid_Samples  Dropped_Samples\n",
      "   Grad_NoCOT        Grad    NoCOT   92.86%         1.0000           0.4097 0.5903       1.000   0.000             14               11\n",
      "   Sal_NoMost         Sal   NoMost   96.00%         1.0000           0.5370 0.4630       1.000   0.000             25                0\n",
      "  Sal_NoLeast         Sal  NoLeast   96.00%         1.0000           0.3473 0.6527       1.000   0.000             25                0\n",
      "    Sal_NoCOT         Sal    NoCOT   92.00%         0.9891           0.7952 0.1940       0.570   0.003             25                0\n",
      "\n",
      "\n",
      "--- Analysis for Question Type: Open Ended ---\n",
      "Configuration Attribution COT_Type Accuracy Mean_P_Correct Mean_P_Incorrect  P_Diff Correlation P_Value  Valid_Samples  Dropped_Samples\n",
      "  Grad_NoMost        Grad   NoMost   84.00%         0.1189           0.0000  0.1189       0.233   0.262             25                0\n",
      "   Grad_NoCOT        Grad    NoCOT   84.00%         0.1189           0.0000  0.1189       0.233   0.262             25                0\n",
      "     Grad_COT        Grad      COT   88.00%         0.1993           0.0000  0.1993       0.224   0.282             25                0\n",
      "  Sal_NoLeast         Sal  NoLeast   88.00%         0.1123           0.0007  0.1116       0.260   0.209             25                0\n",
      "      Sal_COT         Sal      COT   84.00%         0.2030           0.0015  0.2015       0.242   0.243             25                0\n",
      "    Sal_NoCOT         Sal    NoCOT   72.00%         0.0007           0.0000  0.0007       0.156   0.458             25                0\n",
      "   Sal_NoMost         Sal   NoMost   76.00%         0.1015           0.1125 -0.0111      -0.023   0.912             25                0\n",
      "\n",
      "\n",
      "--- Analysis for Question Type: Math ---\n",
      "Warning: Sal_COT for Math has insufficient data for correlation analysis\n",
      "Warning: Sal_NoLeast for Math has insufficient data for correlation analysis\n",
      "Warning: Grad_COT for Math has insufficient data for correlation analysis\n",
      "Configuration Attribution COT_Type Accuracy Mean_P_Correct Mean_P_Incorrect  P_Diff Correlation P_Value  Valid_Samples  Dropped_Samples\n",
      "   Grad_NoCOT        Grad    NoCOT   58.33%         0.1032           0.0000  0.1032       0.337   0.284             12               13\n",
      "  Grad_NoMost        Grad   NoMost   92.86%         0.2386           0.8361 -0.5974      -0.434   0.121             14               11\n",
      "    Sal_NoCOT         Sal    NoCOT   52.00%         0.1057           0.0005  0.1052       0.381   0.060             25                0\n",
      "   Sal_NoMost         Sal   NoMost   68.00%         0.2797           0.2089  0.0708       0.108   0.608             25                0\n",
      "\n",
      "\n",
      "--- Analysis for Question Type: nan ---\n",
      "Warning: Sal_NoCOT for nan has insufficient data for correlation analysis\n",
      "Warning: Sal_COT for nan has insufficient data for correlation analysis\n",
      "Warning: Sal_NoMost for nan has insufficient data for correlation analysis\n",
      "Warning: Sal_NoLeast for nan has insufficient data for correlation analysis\n",
      "Warning: Grad_NoCOT for nan has insufficient data for correlation analysis\n",
      "Warning: Grad_COT for nan has insufficient data for correlation analysis\n",
      "Warning: Grad_NoMost for nan has insufficient data for correlation analysis\n",
      "No results for question type nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# List all configurations\n",
    "configs = [\n",
    "    'Sal_NoCOT', 'Sal_COT', 'Sal_NoMost', 'Sal_NoLeast', \n",
    "    'Grad_NoCOT', 'Grad_COT', 'Grad_NoMost'\n",
    "]\n",
    "\n",
    "# Get unique question types\n",
    "question_types = df['Question_Type'].unique()\n",
    "print(f\"Analyzing {len(question_types)} question types: {question_types}\")\n",
    "\n",
    "# For each question type, create a separate table\n",
    "for q_type in question_types:\n",
    "    print(f\"\\n\\n--- Analysis for Question Type: {q_type} ---\")\n",
    "    \n",
    "    # Filter data for this question type\n",
    "    df_q = df[df['Question_Type'] == q_type]\n",
    "    \n",
    "    # Create empty lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # For each configuration, calculate statistics\n",
    "    for config in configs:\n",
    "        # Skip if the column doesn't exist\n",
    "        if f'{config}_Correctness' not in df_q.columns or f'{config}_P' not in df_q.columns:\n",
    "            continue\n",
    "        \n",
    "        # Clean data by removing NaN and infinite values\n",
    "        valid_data = df_q[[f'{config}_Correctness', f'{config}_P']].dropna()\n",
    "        valid_data = valid_data[~np.isinf(valid_data[f'{config}_P'])]\n",
    "        \n",
    "        # Check if we have enough data and both correct and incorrect samples\n",
    "        correct_samples = valid_data[valid_data[f'{config}_Correctness'] == 1]\n",
    "        incorrect_samples = valid_data[valid_data[f'{config}_Correctness'] == 0]\n",
    "        \n",
    "        if len(correct_samples) > 0 and len(incorrect_samples) > 0:\n",
    "            # Get mean probability for correct and incorrect answers\n",
    "            mean_p_correct = correct_samples[f'{config}_P'].mean()\n",
    "            mean_p_incorrect = incorrect_samples[f'{config}_P'].mean()\n",
    "            \n",
    "            # Calculate correlation between correctness and probability\n",
    "            try:\n",
    "                corr, p_val = pearsonr(valid_data[f'{config}_Correctness'], valid_data[f'{config}_P'])\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Error calculating correlation for {config}: {e}\")\n",
    "                corr, p_val = np.nan, np.nan\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = valid_data[f'{config}_Correctness'].mean()\n",
    "            \n",
    "            # Count of samples\n",
    "            total_samples = len(valid_data)\n",
    "            original_samples = len(df_q)\n",
    "            dropped_samples = original_samples - total_samples\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({\n",
    "                'Configuration': config,\n",
    "                'Attribution': config.split('_')[0],\n",
    "                'COT_Type': '_'.join(config.split('_')[1:]),\n",
    "                'Accuracy': accuracy,\n",
    "                'Mean_P_Correct': mean_p_correct,\n",
    "                'Mean_P_Incorrect': mean_p_incorrect,\n",
    "                'P_Diff': mean_p_correct - mean_p_incorrect,\n",
    "                'Correlation': corr,\n",
    "                'P_Value': p_val,\n",
    "                'Valid_Samples': total_samples,\n",
    "                'Dropped_Samples': dropped_samples\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: {config} for {q_type} has insufficient data for correlation analysis\")\n",
    "    \n",
    "    # Create a DataFrame from results\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Sort by attribution method and then by correlation strength (handle NaN values)\n",
    "        results_df = results_df.sort_values(['Attribution', 'Correlation'], \n",
    "                                            ascending=[True, False], \n",
    "                                            na_position='last')\n",
    "        \n",
    "        # Format for readability\n",
    "        formatted_df = results_df.copy()\n",
    "        formatted_df['Accuracy'] = formatted_df['Accuracy'].map(lambda x: '{:.2%}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        formatted_df['Mean_P_Correct'] = formatted_df['Mean_P_Correct'].map(lambda x: '{:.4f}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        formatted_df['Mean_P_Incorrect'] = formatted_df['Mean_P_Incorrect'].map(lambda x: '{:.4f}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        formatted_df['P_Diff'] = formatted_df['P_Diff'].map(lambda x: '{:.4f}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        formatted_df['Correlation'] = formatted_df['Correlation'].map(lambda x: '{:.3f}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        formatted_df['P_Value'] = formatted_df['P_Value'].map(lambda x: '{:.3f}'.format(x) if not pd.isna(x) else 'N/A')\n",
    "        \n",
    "        # Display the table\n",
    "        print(formatted_df.to_string(index=False))\n",
    "    else:\n",
    "        print(f\"No results for question type {q_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5ddbcd-c1cc-4756-9e3c-6fd45ca1b6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows with both attribution methods: 78 (out of 101 total rows)\n",
      "\n",
      "Overall agreement rate: 79.49% (62/78)\n",
      "\n",
      "Agreement by Question Type:\n",
      "--------------------------\n",
      "T/F: 96.00% (24/25)\n",
      "Multiple Choice: 100.00% (14/14)\n",
      "Open Ended: 52.00% (13/25)\n",
      "Math: 78.57% (11/14)\n",
      "\n",
      "Contingency Table (Sal_MaxStep vs Grad_MaxStep):\n",
      "----------------------------------------------\n",
      "Grad_MaxStep  1.0  2.0  3.0  4.0  5.0\n",
      "Sal_MaxStep                          \n",
      "2.0             0    2    0    1    0\n",
      "3.0             2    3   13    0    0\n",
      "4.0             5    0    0   44    0\n",
      "5.0             1    1    2    1    3\n",
      "\n",
      "Chi-square test: chi2=104.26, p=0.0000\n",
      "\n",
      "Percentage of questions where each step was identified as most important:\n",
      "----------------------------------------------------------------------\n",
      "Saliency method:\n",
      "Step 2.0: 3.85% (3/78)\n",
      "Step 3.0: 23.08% (18/78)\n",
      "Step 4.0: 62.82% (49/78)\n",
      "Step 5.0: 10.26% (8/78)\n",
      "\n",
      "Gradient method:\n",
      "Step 1.0: 10.26% (8/78)\n",
      "Step 2.0: 7.69% (6/78)\n",
      "Step 3.0: 19.23% (15/78)\n",
      "Step 4.0: 58.97% (46/78)\n",
      "Step 5.0: 3.85% (3/78)\n",
      "\n",
      "Step preference by question type:\n",
      "\n",
      "T/F (n=25):\n",
      "Saliency method:\n",
      "  Step 2.0: 4.00% (1/25)\n",
      "  Step 4.0: 96.00% (24/25)\n",
      "Gradient method:\n",
      "  Step 4.0: 100.00% (25/25)\n",
      "\n",
      "Multiple Choice (n=14):\n",
      "Saliency method:\n",
      "  Step 4.0: 100.00% (14/14)\n",
      "Gradient method:\n",
      "  Step 4.0: 100.00% (14/14)\n",
      "\n",
      "Open Ended (n=25):\n",
      "Saliency method:\n",
      "  Step 3.0: 36.00% (9/25)\n",
      "  Step 4.0: 32.00% (8/25)\n",
      "  Step 5.0: 32.00% (8/25)\n",
      "Gradient method:\n",
      "  Step 1.0: 20.00% (5/25)\n",
      "  Step 2.0: 16.00% (4/25)\n",
      "  Step 3.0: 28.00% (7/25)\n",
      "  Step 4.0: 24.00% (6/25)\n",
      "  Step 5.0: 12.00% (3/25)\n",
      "\n",
      "Math (n=14):\n",
      "Saliency method:\n",
      "  Step 2.0: 14.29% (2/14)\n",
      "  Step 3.0: 64.29% (9/14)\n",
      "  Step 4.0: 21.43% (3/14)\n",
      "Gradient method:\n",
      "  Step 1.0: 21.43% (3/14)\n",
      "  Step 2.0: 14.29% (2/14)\n",
      "  Step 3.0: 57.14% (8/14)\n",
      "  Step 4.0: 7.14% (1/14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "#filter rows where we have both Sal_MaxStep and Grad_MaxStep\n",
    "valid_df = df.dropna(subset=['Sal_MaxStep', 'Grad_MaxStep'])\n",
    "\n",
    "# Check how many rows remain after filtering\n",
    "print(f\"Total rows with both attribution methods: {len(valid_df)} (out of {len(df)} total rows)\")\n",
    "\n",
    "# Calculate agreement rate\n",
    "agreement_count = (valid_df['Sal_MaxStep'] == valid_df['Grad_MaxStep']).sum()\n",
    "agreement_rate = agreement_count / len(valid_df)\n",
    "\n",
    "print(f\"\\nOverall agreement rate: {agreement_rate:.2%} ({agreement_count}/{len(valid_df)})\")\n",
    "\n",
    "# break it down by question type\n",
    "question_types = valid_df['Question_Type'].unique()\n",
    "print(\"\\nAgreement by Question Type:\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "for q_type in question_types:\n",
    "    q_df = valid_df[valid_df['Question_Type'] == q_type]\n",
    "    if len(q_df) > 0:\n",
    "        q_agreement = (q_df['Sal_MaxStep'] == q_df['Grad_MaxStep']).sum()\n",
    "        q_agreement_rate = q_agreement / len(q_df)\n",
    "        print(f\"{q_type}: {q_agreement_rate:.2%} ({q_agreement}/{len(q_df)})\")\n",
    "\n",
    "# Create a contingency table to see which steps were identified as most salient\n",
    "print(\"\\nContingency Table (Sal_MaxStep vs Grad_MaxStep):\")\n",
    "print(\"----------------------------------------------\")\n",
    "contingency_table = pd.crosstab(valid_df['Sal_MaxStep'], valid_df['Grad_MaxStep'])\n",
    "print(contingency_table)\n",
    "\n",
    "# Calculate chi-square test to see if there's a significant relationship\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"\\nChi-square test: chi2={chi2:.2f}, p={p:.4f}\")\n",
    "\n",
    "# Create a visualization of the agreement\n",
    "plt.figure(figsize=(10, 6))\n",
    "agreement_by_type = []\n",
    "for q_type in question_types:\n",
    "    q_df = valid_df[valid_df['Question_Type'] == q_type]\n",
    "    if len(q_df) > 0:\n",
    "        q_agreement_rate = (q_df['Sal_MaxStep'] == q_df['Grad_MaxStep']).mean()\n",
    "        agreement_by_type.append({'Question Type': q_type, 'Agreement Rate': q_agreement_rate})\n",
    "\n",
    "agreement_df = pd.DataFrame(agreement_by_type)\n",
    "agreement_df = agreement_df.sort_values('Agreement Rate', ascending=False)\n",
    "\n",
    "# Calculate percentage of questions where each step was identified as most important\n",
    "print(\"\\nPercentage of questions where each step was identified as most important:\")\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "total_questions = len(valid_df)\n",
    "\n",
    "print(\"Saliency method:\")\n",
    "sal_step_counts = valid_df['Sal_MaxStep'].value_counts().sort_index()\n",
    "for step, count in sal_step_counts.items():\n",
    "    print(f\"Step {step}: {count/total_questions:.2%} ({count}/{total_questions})\")\n",
    "\n",
    "print(\"\\nGradient method:\")\n",
    "grad_step_counts = valid_df['Grad_MaxStep'].value_counts().sort_index()\n",
    "for step, count in grad_step_counts.items():\n",
    "    print(f\"Step {step}: {count/total_questions:.2%} ({count}/{total_questions})\")\n",
    "\n",
    "# Let's also include a more detailed analysis of step preference by question type\n",
    "print(\"\\nStep preference by question type:\")\n",
    "for q_type in question_types:\n",
    "    q_df = valid_df[valid_df['Question_Type'] == q_type]\n",
    "    if len(q_df) > 0:\n",
    "        print(f\"\\n{q_type} (n={len(q_df)}):\")\n",
    "        \n",
    "        print(\"Saliency method:\")\n",
    "        q_sal_counts = q_df['Sal_MaxStep'].value_counts().sort_index()\n",
    "        for step, count in q_sal_counts.items():\n",
    "            print(f\"  Step {step}: {count/len(q_df):.2%} ({count}/{len(q_df)})\")\n",
    "        \n",
    "        print(\"Gradient method:\")\n",
    "        q_grad_counts = q_df['Grad_MaxStep'].value_counts().sort_index()\n",
    "        for step, count in q_grad_counts.items():\n",
    "            print(f\"  Step {step}: {count/len(q_df):.2%} ({count}/{len(q_df)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ce19a-217a-48d5-a32a-4544205caac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Comparison: Saliency vs Gradient Attribution\n",
      "--------------------------------------------------------\n",
      "  Question Type  Count  Sal Avg Drop  Grad Avg Drop  Sal Faithful %  Grad Faithful % More Faithful Method\n",
      "            T/F     25         0.120         -0.040            12.0             12.0             Saliency\n",
      "Multiple Choice     14         0.071          0.000             7.1              0.0             Saliency\n",
      "     Open Ended     25         0.080          0.040             8.0              8.0             Saliency\n",
      "           Math     14         0.429          0.071            42.9              7.1             Saliency\n",
      "\n",
      "Final Results (including overall):\n",
      "  Question Type  Count  Sal Avg Drop  Grad Avg Drop  Sal Faithful %  Grad Faithful % More Faithful Method\n",
      "            T/F     25         0.120         -0.040            12.0             12.0             Saliency\n",
      "Multiple Choice     14         0.071          0.000             7.1              0.0             Saliency\n",
      "     Open Ended     25         0.080          0.040             8.0              8.0             Saliency\n",
      "           Math     14         0.429          0.071            42.9              7.1             Saliency\n",
      "        OVERALL     78         0.154          0.013            15.4              7.7             Saliency\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "\n",
    "# Get unique question types\n",
    "question_types = df['Question_Type'].unique()\n",
    "\n",
    "for q_type in question_types:\n",
    "    # Filter for this question type\n",
    "    q_df = df[df['Question_Type'] == q_type]\n",
    "    \n",
    "    # Drop rows with missing values in key columns\n",
    "    clean_df = q_df.dropna(subset=[\n",
    "        'Sal_COT_Correctness', 'Sal_NoMost_Correctness',\n",
    "        'Grad_COT_Correctness', 'Grad_NoMost_Correctness'\n",
    "    ])\n",
    "    \n",
    "    if len(clean_df) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate performance drops when removing most important step\n",
    "    sal_drop = clean_df['Sal_COT_Correctness'] - clean_df['Sal_NoMost_Correctness']\n",
    "    grad_drop = clean_df['Grad_COT_Correctness'] - clean_df['Grad_NoMost_Correctness']\n",
    "    \n",
    "    # Calculate average drops\n",
    "    avg_sal_drop = sal_drop.mean()\n",
    "    avg_grad_drop = grad_drop.mean()\n",
    "    \n",
    "    # Count cases where removing the step hurts performance\n",
    "    sal_faithful_count = (sal_drop > 0).sum()\n",
    "    grad_faithful_count = (grad_drop > 0).sum()\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Question Type': q_type,\n",
    "        'Count': len(clean_df),\n",
    "        'Sal Avg Drop': round(avg_sal_drop, 3),\n",
    "        'Grad Avg Drop': round(avg_grad_drop, 3),\n",
    "        'Sal Faithful %': round(sal_faithful_count / len(clean_df) * 100, 1),\n",
    "        'Grad Faithful %': round(grad_faithful_count / len(clean_df) * 100, 1),\n",
    "        'More Faithful Method': 'Saliency' if avg_sal_drop > avg_grad_drop else 'Gradient'\n",
    "    })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "faithfulness_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Faithfulness Comparison: Saliency vs Gradient Attribution\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(faithfulness_df.to_string(index=False))\n",
    "\n",
    "# Calculate overall results\n",
    "overall = {\n",
    "    'Question Type': 'OVERALL',\n",
    "    'Count': faithfulness_df['Count'].sum(),\n",
    "    'Sal Avg Drop': round(np.average(faithfulness_df['Sal Avg Drop'], weights=faithfulness_df['Count']), 3),\n",
    "    'Grad Avg Drop': round(np.average(faithfulness_df['Grad Avg Drop'], weights=faithfulness_df['Count']), 3)\n",
    "}\n",
    "\n",
    "# Calculate overall faithful percentages\n",
    "weighted_sal_faithful = sum(row['Sal Faithful %'] * row['Count'] for _, row in faithfulness_df.iterrows()) / overall['Count']\n",
    "weighted_grad_faithful = sum(row['Grad Faithful %'] * row['Count'] for _, row in faithfulness_df.iterrows()) / overall['Count']\n",
    "\n",
    "overall['Sal Faithful %'] = round(weighted_sal_faithful, 1)\n",
    "overall['Grad Faithful %'] = round(weighted_grad_faithful, 1)\n",
    "overall['More Faithful Method'] = 'Saliency' if overall['Sal Avg Drop'] > overall['Grad Avg Drop'] else 'Gradient'\n",
    "\n",
    "# Add overall row to the bottom\n",
    "faithfulness_df = pd.concat([faithfulness_df, pd.DataFrame([overall])], ignore_index=True)\n",
    "\n",
    "print(\"\\nFinal Results (including overall):\")\n",
    "print(faithfulness_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f60fa6-d473-4dc5-a8c0-e15eca41d3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0a1ad6-eeb1-47de-a815-841328350959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
